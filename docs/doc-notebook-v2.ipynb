{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocumentaciÃ³n: VersiÃ³n 2 - Lighthouse Assistant\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Resumen de Cambios\n",
    "\n",
    "### Nuevas Funcionalidades Implementadas\n",
    "\n",
    "Se han implementado tres grandes mejoras al sistema:\n",
    "\n",
    "1. **Sistema de Carga de Reportes**: Los usuarios pueden subir mÃºltiples reportes JSON de Google Lighthouse\n",
    "2. **Preprocesamiento Inteligente**: ReducciÃ³n automÃ¡tica del tamaÃ±o de reportes (20-95%)\n",
    "3. **Resumen con LLM**: ConversiÃ³n de JSON a resÃºmenes en lenguaje natural usando un modelo secundario\n",
    "4. **Arquitectura Multi-Modelo**: Uso de dos modelos LLaMA para optimizar costos y calidad\n",
    "5. **Control de temperatura**: El usuario puede ajustar la creatividad del resumen generado\n",
    "\n",
    "### Beneficios\n",
    "\n",
    "- âœ… **ReducciÃ³n drÃ¡stica de tokens**: De cientos de miles a menos de 5000\n",
    "- âœ… **OptimizaciÃ³n de costos**: Usa modelo pequeÃ±o (8b-instant) para tareas de resumen\n",
    "- âœ… **Mejor calidad**: El modelo grande recibe informaciÃ³n pre-procesada\n",
    "- âœ… **Escalabilidad**: Maneja reportes de cualquier tamaÃ±o\n",
    "- âœ… **Experiencia mejorada**: Feedback visual en tiempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sistema de Carga de Reportes JSON\n",
    "\n",
    "### `ui/layout.py` - Modificaciones\n",
    "\n",
    "Se agregÃ³ funcionalidad completa para cargar, procesar y gestionar reportes JSON de Google Lighthouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import json\n",
    "\n",
    "\n",
    "def render_layout():\n",
    "    \"\"\"\n",
    "    Renderiza el layout con funcionalidad de carga de reportes.\n",
    "    \n",
    "    Nuevas caracterÃ­sticas:\n",
    "    - Sube mÃºltiples archivos JSON\n",
    "    - Valida formato JSON\n",
    "    - Almacena en session_state\n",
    "    - Muestra lista de reportes cargados\n",
    "    - Permite eliminar reportes individuales\n",
    "    \"\"\"\n",
    "    \n",
    "    st.set_page_config(page_title=\"Google Lighthouse Assistant\", layout=\"wide\")\n",
    "    st.title(\"Asistente :blue[Google Lighthouse]\")\n",
    "    \n",
    "    with st.sidebar:\n",
    "        st.subheader(\"Reportes Lighthouse\")\n",
    "        \n",
    "        # 1. File uploader con soporte para mÃºltiples archivos\n",
    "        uploaded_files = st.file_uploader(\n",
    "            \"Carga tus reportes JSON\",\n",
    "            type=\"json\",\n",
    "            accept_multiple_files=True  # â† NUEVO: mÃºltiples archivos\n",
    "        )\n",
    "        \n",
    "        # 2. Procesar archivos subidos\n",
    "        if uploaded_files:\n",
    "            if \"lighthouse_reports\" not in st.session_state:\n",
    "                st.session_state.lighthouse_reports = {}\n",
    "            \n",
    "            for uploaded_file in uploaded_files:\n",
    "                try:\n",
    "                    # Leer y parsear JSON\n",
    "                    report_data = json.load(uploaded_file)\n",
    "                    file_name = uploaded_file.name\n",
    "                    \n",
    "                    # Almacenar si es nuevo\n",
    "                    if file_name not in st.session_state.lighthouse_reports:\n",
    "                        st.session_state.lighthouse_reports[file_name] = report_data\n",
    "                        st.session_state.report_loaded = True  # â† Flag para notificaciÃ³n\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    st.error(f\"Error al leer {uploaded_file.name}: no es un JSON vÃ¡lido\")\n",
    "        \n",
    "        # 3. Mostrar reportes cargados\n",
    "        if \"lighthouse_reports\" in st.session_state and st.session_state.lighthouse_reports:\n",
    "            st.success(f\"âœ… {len(st.session_state.lighthouse_reports)} reporte(s) cargado(s)\")\n",
    "            \n",
    "            # Listar con opciÃ³n de eliminar\n",
    "            for file_name in list(st.session_state.lighthouse_reports.keys()):\n",
    "                col1, col2 = st.columns([3, 1])\n",
    "                with col1:\n",
    "                    st.text(f\"ğŸ“„ {file_name}\")\n",
    "                with col2:\n",
    "                    if st.button(\"ğŸ—‘ï¸\", key=f\"delete_{file_name}\"):\n",
    "                        del st.session_state.lighthouse_reports[file_name]\n",
    "                        st.session_state.report_removed = True  # â† Flag para notificaciÃ³n\n",
    "                        st.rerun()\n",
    "        else:\n",
    "            st.info(\"No hay reportes cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session State Variables\n",
    "\n",
    "\n",
    "Se agregaron nuevas variables al session state de Streamlit:\n",
    "\n",
    "- **`st.session_state.lighthouse_reports`**: Diccionario {filename: report_data}\n",
    "- **`st.session_state.report_loaded`**: Flag booleano que indica cuando se cargÃ³ un nuevo reporte\n",
    "- **`st.session_state.report_removed`**: Flag booleano que indica cuando se eliminÃ³ un reporte\n",
    "\n",
    "Estos flags permiten mostrar mensajes de confirmaciÃ³n en el chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ui/chat.py` - Notificaciones\n",
    "\n",
    "Se agregÃ³ lÃ³gica para mostrar mensajes de confirmaciÃ³n cuando se cargan/eliminan reportes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from core.model import get_model_response\n",
    "\n",
    "\n",
    "def render_chat():\n",
    "    \"\"\"\n",
    "    Renderiza el chat con notificaciones automÃ¡ticas de carga/eliminaciÃ³n de reportes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    \n",
    "    # NUEVO: Detectar carga de reportes\n",
    "    if st.session_state.get(\"report_loaded\", False):\n",
    "        num_reports = len(st.session_state.get(\"lighthouse_reports\", {}))\n",
    "        report_names = list(st.session_state.get(\"lighthouse_reports\", {}).keys())\n",
    "        \n",
    "        if num_reports == 1:\n",
    "            msg = f\"ğŸ“Š Reporte cargado: **{report_names[0]}**. Ahora puedes preguntarme sobre el anÃ¡lisis.\"\n",
    "        else:\n",
    "            msg = f\"ğŸ“Š {num_reports} reportes cargados. Puedo analizar cualquiera de ellos.\"\n",
    "        \n",
    "        # Agregar mensaje al historial (se renderizarÃ¡ despuÃ©s)\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": msg})\n",
    "        st.session_state.report_loaded = False  # Reset flag\n",
    "    \n",
    "    # NUEVO: Detectar eliminaciÃ³n de reportes\n",
    "    if st.session_state.get(\"report_removed\", False):\n",
    "        remaining = len(st.session_state.get(\"lighthouse_reports\", {}))\n",
    "        if remaining > 0:\n",
    "            msg = f\"ğŸ—‘ï¸ Reporte eliminado. Quedan {remaining} reporte(s) cargado(s).\"\n",
    "        else:\n",
    "            msg = \"ğŸ—‘ï¸ Reporte eliminado. No hay reportes cargados actualmente.\"\n",
    "        \n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": msg})\n",
    "        st.session_state.report_removed = False  # Reset flag\n",
    "    \n",
    "    # Mostrar historial de mensajes\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "    \n",
    "    # Input del usuario\n",
    "    if prompt := st.chat_input():\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"Pensando...\"):\n",
    "                # NUEVO: Pasar reportes al modelo\n",
    "                lighthouse_reports = st.session_state.get(\"lighthouse_reports\", {})\n",
    "                response = get_model_response(st.session_state.messages, lighthouse_reports)\n",
    "                st.markdown(response)\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Reportes\n",
    "\n",
    "### `core/model.py` - FunciÃ³n `preprocess_lighthouse_report()`\n",
    "\n",
    "Esta funciÃ³n reduce el tamaÃ±o de los reportes JSON eliminando datos innecesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lighthouse_report(report: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Reduce el tamaÃ±o del reporte de Lighthouse manteniendo solo informaciÃ³n esencial.\n",
    "    \n",
    "    MANTIENE:\n",
    "    - categories.*.score, title, description\n",
    "    - audits.*.score, title, description, displayValue\n",
    "    - audits.*.details.summary (si existe)\n",
    "    - MÃ©tricas principales: finalUrl, requestedUrl, timing, configSettings\n",
    "    \n",
    "    ELIMINA:\n",
    "    - audits.*.details.items (arrays grandes)\n",
    "    - full-page-screenshot\n",
    "    - traces\n",
    "    - network-requests completos\n",
    "    - Strings > 5000 caracteres\n",
    "    \n",
    "    ReducciÃ³n esperada: 20-95% dependiendo del reporte\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    \n",
    "    # 1. Campos principales\n",
    "    top_level_fields = [\n",
    "        \"finalUrl\", \"requestedUrl\", \"fetchTime\",\n",
    "        \"userAgent\", \"environment\", \"runtimeError\", \"timing\"\n",
    "    ]\n",
    "    for field in top_level_fields:\n",
    "        if field in report:\n",
    "            processed[field] = report[field]\n",
    "    \n",
    "    # 2. ConfigSettings (solo lo esencial)\n",
    "    if \"configSettings\" in report:\n",
    "        config = report[\"configSettings\"]\n",
    "        processed[\"configSettings\"] = {\n",
    "            \"emulatedFormFactor\": config.get(\"emulatedFormFactor\"),\n",
    "            \"locale\": config.get(\"locale\"),\n",
    "            \"onlyCategories\": config.get(\"onlyCategories\"),\n",
    "        }\n",
    "    \n",
    "    # 3. Categories (scores y tÃ­tulos)\n",
    "    if \"categories\" in report:\n",
    "        processed[\"categories\"] = {}\n",
    "        for category_id, category_data in report[\"categories\"].items():\n",
    "            processed[\"categories\"][category_id] = {\n",
    "                \"id\": category_data.get(\"id\"),\n",
    "                \"title\": category_data.get(\"title\"),\n",
    "                \"score\": category_data.get(\"score\"),\n",
    "                \"description\": category_data.get(\"description\"),\n",
    "            }\n",
    "    \n",
    "    # 4. Audits (sin items completos)\n",
    "    if \"audits\" in report:\n",
    "        processed[\"audits\"] = {}\n",
    "        for audit_id, audit_data in report[\"audits\"].items():\n",
    "            processed_audit = {\n",
    "                \"id\": audit_data.get(\"id\"),\n",
    "                \"title\": audit_data.get(\"title\"),\n",
    "                \"description\": audit_data.get(\"description\"),\n",
    "                \"score\": audit_data.get(\"score\"),\n",
    "                \"scoreDisplayMode\": audit_data.get(\"scoreDisplayMode\"),\n",
    "                \"displayValue\": audit_data.get(\"displayValue\"),\n",
    "                \"numericValue\": audit_data.get(\"numericValue\"),\n",
    "                \"numericUnit\": audit_data.get(\"numericUnit\"),\n",
    "            }\n",
    "            \n",
    "            # Procesar details (solo summary, NO items)\n",
    "            if \"details\" in audit_data and isinstance(audit_data[\"details\"], dict):\n",
    "                details = audit_data[\"details\"]\n",
    "                processed_details = {}\n",
    "                \n",
    "                if \"summary\" in details:\n",
    "                    processed_details[\"summary\"] = details[\"summary\"]\n",
    "                \n",
    "                if \"type\" in details:\n",
    "                    processed_details[\"type\"] = details[\"type\"]\n",
    "                \n",
    "                # Contar items en lugar de incluirlos\n",
    "                if \"items\" in details and isinstance(details[\"items\"], list):\n",
    "                    processed_details[\"itemsCount\"] = len(details[\"items\"])\n",
    "                \n",
    "                if processed_details:\n",
    "                    processed_audit[\"details\"] = processed_details\n",
    "            \n",
    "            processed[\"audits\"][audit_id] = processed_audit\n",
    "    \n",
    "    # 5. Eliminar valores muy largos\n",
    "    processed = _remove_large_values(processed, max_length=5000)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FunciÃ³n auxiliar `_remove_large_values()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_large_values(obj: Any, max_length: int = 5000) -> Any:\n",
    "    \"\"\"\n",
    "    Recursivamente elimina valores cuya representaciÃ³n en string supere max_length.\n",
    "    \n",
    "    TambiÃ©n elimina campos conocidos que son muy grandes:\n",
    "    - full-page-screenshot\n",
    "    - screenshot, screenshots\n",
    "    - trace, traces\n",
    "    - network-requests\n",
    "    - items (ya manejado en preprocess)\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        result = {}\n",
    "        for key, value in obj.items():\n",
    "            # Saltar campos conocidos como grandes\n",
    "            if key in [\n",
    "                \"full-page-screenshot\", \"screenshot\", \"screenshots\",\n",
    "                \"trace\", \"traces\", \"network-requests\", \"items\"\n",
    "            ]:\n",
    "                continue\n",
    "            \n",
    "            processed_value = _remove_large_values(value, max_length)\n",
    "            \n",
    "            # Reemplazar strings muy largos\n",
    "            if isinstance(processed_value, str) and len(processed_value) > max_length:\n",
    "                result[key] = f\"[Valor muy largo - {len(processed_value)} caracteres]\"\n",
    "            elif processed_value is not None:\n",
    "                result[key] = processed_value\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    elif isinstance(obj, list):\n",
    "        return [_remove_large_values(item, max_length) for item in obj]\n",
    "    \n",
    "    elif isinstance(obj, str):\n",
    "        if len(obj) > max_length:\n",
    "            return f\"[Texto muy largo - {len(obj)} caracteres]\"\n",
    "        return obj\n",
    "    \n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de ReducciÃ³n\n",
    "\n",
    "```\n",
    "Reporte Original:        2,500,000 caracteres (2.5 MB)\n",
    "         â†“\n",
    "Reporte Preprocesado:      250,000 caracteres (250 KB)\n",
    "         â†“\n",
    "ReducciÃ³n:                 90% eliminado\n",
    "```\n",
    "\n",
    "**Campos eliminados mÃ¡s grandes:**\n",
    "- `full-page-screenshot`: ~500-1500 KB (base64 de imÃ¡genes)\n",
    "- `audits.*.details.items`: ~300-800 KB (arrays con cientos de entradas)\n",
    "- `network-requests`: ~100-200 KB\n",
    "- `trace`: ~200-500 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sistema de Resumen con LLM\n",
    "\n",
    "### `core/model.py` - FunciÃ³n `summarize_preprocessed_report()`\n",
    "\n",
    "Esta funciÃ³n convierte el JSON preprocesado en un resumen en lenguaje natural usando `llama-3.1-8b-instant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_preprocessed_report(preprocessed: dict) -> str:\n",
    "    \"\"\"\n",
    "    Resume un reporte preprocesado de Lighthouse en texto conciso.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Convierte JSON a texto\n",
    "    2. Divide en chunks de 3000 caracteres\n",
    "    3. EnvÃ­a cada chunk a llama-3.1-8b-instant\n",
    "    4. Fusiona resÃºmenes en texto < 5000 tokens\n",
    "    \n",
    "    Returns:\n",
    "        str: Resumen en texto para usar como contexto\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        \n",
    "        # 1. Convertir JSON a texto\n",
    "        report_text = json.dumps(preprocessed, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 2. Dividir en chunks de 3000 caracteres\n",
    "        chunk_size = 3000\n",
    "        chunks = []\n",
    "        for i in range(0, len(report_text), chunk_size):\n",
    "            chunks.append(report_text[i : i + chunk_size])\n",
    "        \n",
    "        # 3. Prompt para resumir\n",
    "        summary_prompt = \"\"\"Resume este fragmento del reporte de Lighthouse manteniendo solo:\n",
    "- problemas principales\n",
    "- mÃ©tricas clave (performance, SEO, accesibilidad)\n",
    "- oportunidades de mejora\n",
    "- puntuaciones relevantes\n",
    "MÃ¡ximo 800 tokens.\"\"\"\n",
    "        \n",
    "        # 4. Resumir cada chunk con modelo pequeÃ±o\n",
    "        chunk_summaries = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": summary_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Fragmento {idx + 1} de {len(chunks)}:\\n\\n{chunk}\"},\n",
    "            ]\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama-3.1-8b-instant\",  # â† Modelo pequeÃ±o y rÃ¡pido\n",
    "                messages=messages,\n",
    "                temperature=0.3,                # â† MÃ¡s determinista\n",
    "                max_tokens=800,                 # â† LÃ­mite por chunk\n",
    "                top_p=1,\n",
    "                stream=False,\n",
    "            )\n",
    "            \n",
    "            chunk_summaries.append(response.choices[0].message.content)\n",
    "        \n",
    "        # 5. Fusionar resÃºmenes\n",
    "        if len(chunk_summaries) == 1:\n",
    "            final_summary = chunk_summaries[0]\n",
    "        else:\n",
    "            combined_summaries = \"\\n\\n---\\n\\n\".join(chunk_summaries)\n",
    "            \n",
    "            # Si el combinado es muy largo, hacer resumen final\n",
    "            if len(combined_summaries) > 15000:  # ~5000 tokens\n",
    "                fusion_prompt = \"\"\"Fusiona estos resÃºmenes del reporte de Lighthouse en un solo resumen coherente.\n",
    "                                MantÃ©n:\n",
    "                                - Todas las puntuaciones de categorÃ­as principales\n",
    "                                - Problemas crÃ­ticos identificados\n",
    "                                - MÃ©tricas clave de rendimiento\n",
    "                                - Oportunidades de mejora mÃ¡s importantes\n",
    "                                MÃ¡ximo 1500 tokens.\"\"\"\n",
    "                \n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": fusion_prompt},\n",
    "                    {\"role\": \"user\", \"content\": combined_summaries},\n",
    "                ]\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"llama-3.1-8b-instant\",\n",
    "                    messages=messages,\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=1500,  # â† LÃ­mite para fusiÃ³n final\n",
    "                    top_p=1,\n",
    "                    stream=False,\n",
    "                )\n",
    "                \n",
    "                final_summary = response.choices[0].message.content\n",
    "            else:\n",
    "                final_summary = combined_summaries\n",
    "        \n",
    "        return final_summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Fallback: informaciÃ³n bÃ¡sica\n",
    "        categories_info = \"\"\n",
    "        if \"categories\" in preprocessed:\n",
    "            categories_info = \"\\n\\nCategorÃ­as:\\n\"\n",
    "            for cat_id, cat_data in preprocessed[\"categories\"].items():\n",
    "                score = cat_data.get(\"score\", 0) * 100\n",
    "                title = cat_data.get(\"title\", cat_id)\n",
    "                categories_info += f\"- {title}: {score:.0f}/100\\n\"\n",
    "        \n",
    "        return f\"Error al resumir reporte: {str(e)}\\n\\nInformaciÃ³n bÃ¡sica:{categories_info}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParÃ¡metros del Modelo para Resumen\n",
    "\n",
    "| ParÃ¡metro | Valor | RazÃ³n |\n",
    "|-----------|-------|-------|\n",
    "| `model` | `llama-3.1-8b-instant` | RÃ¡pido y econÃ³mico para tareas de resumen |\n",
    "| `temperature` | `0.3` | MÃ¡s determinista, resÃºmenes consistentes |\n",
    "| `max_tokens` (chunk) | `800` | Resumen conciso por cada fragmento |\n",
    "| `max_tokens` (fusiÃ³n) | `1500` | Resumen final mÃ¡s completo |\n",
    "| `top_p` | `1` | Sin restricciÃ³n adicional de muestreo |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Arquitectura Multi-Modelo\n",
    "\n",
    "### Modelos Utilizados\n",
    "\n",
    "El sistema ahora usa **dos modelos diferentes de LLaMA** para optimizar costos y calidad:\n",
    "\n",
    "#### Modelo 1: llama-3.1-8b-instant (Resumen)\n",
    "\n",
    "**CaracterÃ­sticas:**\n",
    "- 8 mil millones de parÃ¡metros\n",
    "- Inferencia ultra-rÃ¡pida (\"instant\")\n",
    "- EconÃ³mico en uso de tokens\n",
    "\n",
    "**Uso:**\n",
    "- Procesamiento intermedio\n",
    "- GeneraciÃ³n de resÃºmenes de reportes\n",
    "- ConversiÃ³n de JSON a texto natural\n",
    "\n",
    "**ConfiguraciÃ³n:**\n",
    "```python\n",
    "temperature=0.3,    # Determinista\n",
    "max_tokens=800      # ResÃºmenes concisos\n",
    "```\n",
    "\n",
    "#### Modelo 2: llama-3.3-70b-versatile (AnÃ¡lisis)\n",
    "\n",
    "**CaracterÃ­sticas:**\n",
    "- 70 mil millones de parÃ¡metros\n",
    "- MÃ¡s capaz y versÃ¡til\n",
    "- Mejor comprensiÃ³n contextual\n",
    "\n",
    "**Uso:**\n",
    "- Respuestas finales al usuario\n",
    "- AnÃ¡lisis complejos\n",
    "- Recomendaciones expertas\n",
    "\n",
    "**ConfiguraciÃ³n:**\n",
    "```python\n",
    "temperature=0.7,    # MÃ¡s creativo\n",
    "max_tokens=2000     # Respuestas detalladas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComparaciÃ³n de Modelos\n",
    "\n",
    "| Aspecto | 8b-instant | 70b-versatile |\n",
    "|---------|------------|---------------|\n",
    "| **TamaÃ±o** | 8B parÃ¡metros | 70B parÃ¡metros |\n",
    "| **Velocidad** | Ultra-rÃ¡pido | Moderado |\n",
    "| **Costo** | Bajo | Alto |\n",
    "| **Capacidad** | Buena para tareas simples | Excelente para anÃ¡lisis complejos |\n",
    "| **Uso en app** | Resumen de reportes | Respuestas al usuario |\n",
    "| **Temperature** | 0.3 (determinista) | 0.7 (creativo) |\n",
    "| **Max Tokens** | 800-1500 | 2000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de la Arquitectura Multi-Modelo\n",
    "\n",
    "1. **OptimizaciÃ³n de Costos**: Usa modelo pequeÃ±o para tareas rutinarias\n",
    "2. **Mejor Rendimiento**: Modelo pequeÃ±o procesa mÃ¡s rÃ¡pido\n",
    "3. **Mayor Calidad**: Modelo grande se enfoca en respuestas complejas\n",
    "4. **Escalabilidad**: Cada modelo maneja lo que mejor sabe hacer\n",
    "5. **Eficiencia de Tokens**: Resumen reduce contexto para modelo grande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Flujo Completo de Procesamiento\n",
    "\n",
    "### Pipeline de 3 Etapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              USUARIO SUBE REPORTE JSON                       â”‚\n",
    "â”‚           (2.5 MB - Lighthouse completo)                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â†“\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   ETAPA 1: PREPROCESAMIENTO       â”‚\n",
    "        â”‚   preprocess_lighthouse_report()  â”‚\n",
    "        â”‚                                    â”‚\n",
    "        â”‚   â€¢ Elimina screenshots           â”‚\n",
    "        â”‚   â€¢ Elimina items arrays          â”‚\n",
    "        â”‚   â€¢ Elimina traces                â”‚\n",
    "        â”‚   â€¢ Mantiene scores/tÃ­tulos       â”‚\n",
    "        â”‚   â€¢ Mantiene summaries            â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â†“\n",
    "        Reporte Preprocesado (250 KB)\n",
    "        ReducciÃ³n: 90%\n",
    "                     â”‚\n",
    "                     â†“\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   ETAPA 2: RESUMEN CON LLM        â”‚\n",
    "        â”‚   summarize_preprocessed_report() â”‚\n",
    "        â”‚                                    â”‚\n",
    "        â”‚   â€¢ Convierte JSON a texto        â”‚\n",
    "        â”‚   â€¢ Divide en chunks (3000 chars) â”‚\n",
    "        â”‚   â€¢ EnvÃ­a a llama-3.1-8b-instant  â”‚\n",
    "        â”‚   â€¢ Fusiona resÃºmenes             â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â†“\n",
    "        Resumen en Texto (~5 KB)\n",
    "        < 5000 tokens\n",
    "                     â”‚\n",
    "                     â†“\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   ETAPA 3: ANÃLISIS FINAL         â”‚\n",
    "        â”‚   get_model_response()            â”‚\n",
    "        â”‚                                    â”‚\n",
    "        â”‚   â€¢ Recibe pregunta del usuario   â”‚\n",
    "        â”‚   â€¢ Incluye resumen en contexto   â”‚\n",
    "        â”‚   â€¢ EnvÃ­a a llama-3.3-70b         â”‚\n",
    "        â”‚   â€¢ Genera respuesta experta      â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â†“\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   RESPUESTA AL USUARIO             â”‚\n",
    "        â”‚   AnÃ¡lisis detallado y experto    â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModificaciÃ³n en `get_model_response()`\n",
    "\n",
    "La funciÃ³n principal ahora integra todo el pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(messages: list[dict], lighthouse_reports: dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Obtiene respuesta del modelo principal con reportes resumidos en el contexto.\n",
    "    \n",
    "    Args:\n",
    "        messages: Historial de conversaciÃ³n\n",
    "        lighthouse_reports: Dict de reportes JSON {filename: report_data}\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta del modelo o mensaje de error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "        \n",
    "        # NUEVO: Si hay reportes, preprocesar y resumir\n",
    "        if lighthouse_reports:\n",
    "            reports_context = \"\\n\\n## REPORTES LIGHTHOUSE DISPONIBLES\\n\\n\"\n",
    "            reports_context += (\n",
    "                \"El usuario ha cargado los siguientes reportes de Google Lighthouse. \"\n",
    "                \"A continuaciÃ³n se presenta un resumen de cada reporte:\\n\\n\"\n",
    "            )\n",
    "            \n",
    "            for file_name, report_data in lighthouse_reports.items():\n",
    "                # Etapa 1: Preprocesar\n",
    "                processed_report = preprocess_lighthouse_report(report_data)\n",
    "                \n",
    "                # Etapa 2: Resumir\n",
    "                summary = summarize_preprocessed_report(processed_report)\n",
    "                \n",
    "                # Agregar al contexto\n",
    "                reports_context += f\"### Reporte: {file_name}\\n\\n\"\n",
    "                reports_context += summary\n",
    "                reports_context += \"\\n\\n---\\n\\n\"\n",
    "            \n",
    "            reports_context += (\n",
    "                \"\\nUsa estos resÃºmenes para responder las preguntas del usuario. \"\n",
    "                \"Si el usuario hace una pregunta que requiere anÃ¡lisis de un reporte \"\n",
    "                \"y ya tienes reportes cargados, analÃ­zalos automÃ¡ticamente. \"\n",
    "                \"Si el usuario pregunta algo que no requiere un reporte especÃ­fico, \"\n",
    "                \"responde normalmente con tus conocimientos sobre optimizaciÃ³n web.\\n\\n\"\n",
    "                \"NOTA: Los resÃºmenes incluyen las mÃ©tricas clave, problemas principales \"\n",
    "                \"y oportunidades de mejora identificadas en los reportes.\"\n",
    "            )\n",
    "            \n",
    "            # Agregar contexto al system prompt\n",
    "            enhanced_system_prompt = SYSTEM_PROMPT + \"\\n\\n\" + reports_context\n",
    "            system_message = {\"role\": \"system\", \"content\": enhanced_system_prompt}\n",
    "        \n",
    "        all_messages = [system_message] + messages\n",
    "        \n",
    "        # Etapa 3: AnÃ¡lisis final con modelo grande\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",  # â† Modelo grande\n",
    "            messages=all_messages,\n",
    "            temperature=0.7,                   # â† MÃ¡s creativo\n",
    "            max_tokens=2000,                   # â† Respuestas detalladas\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Lo siento, ha ocurrido un error al procesar tu solicitud: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Session State Management\n",
    "\n",
    "### Variables de Session State\n",
    "\n",
    "Streamlit usa `st.session_state` para mantener datos entre re-ejecuciones de la app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables existentes\n",
    "st.session_state.messages = []  # Historial de chat\n",
    "# Estructura: [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "\n",
    "# NUEVAS variables para reportes\n",
    "st.session_state.lighthouse_reports = {}  # Reportes cargados\n",
    "# Estructura: {\"reporte1.json\": {<datos JSON>}, \"reporte2.json\": {<datos JSON>}}\n",
    "\n",
    "st.session_state.report_loaded = False    # Flag: se cargÃ³ un nuevo reporte\n",
    "st.session_state.report_removed = False   # Flag: se eliminÃ³ un reporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de Vida de los Flags\n",
    "\n",
    "```python\n",
    "# 1. Usuario sube archivo\n",
    "st.session_state.lighthouse_reports[\"reporte.json\"] = data\n",
    "st.session_state.report_loaded = True  # Activar flag\n",
    "\n",
    "# 2. render_chat() detecta el flag\n",
    "if st.session_state.get(\"report_loaded\", False):\n",
    "    # Mostrar mensaje de confirmaciÃ³n\n",
    "    st.session_state.messages.append({...})\n",
    "    st.session_state.report_loaded = False  # Reset flag\n",
    "\n",
    "# 3. Flag se resetea, no se mostrarÃ¡ de nuevo hasta prÃ³xima carga\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejemplos de Uso\n",
    "\n",
    "### Ejemplo 1: Cargar y Analizar un Reporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 1: Usuario sube `mi-sitio.json` via sidebar**\n",
    "\n",
    "```\n",
    "Sistema:\n",
    "  1. Valida JSON âœ“\n",
    "  2. Almacena en session_state.lighthouse_reports\n",
    "  3. Activa flag report_loaded\n",
    "  4. Muestra en sidebar: \"âœ… 1 reporte(s) cargado(s)\"\n",
    "```\n",
    "\n",
    "**Paso 2: Chat muestra mensaje automÃ¡tico**\n",
    "\n",
    "```\n",
    "Assistant: ğŸ“Š Reporte cargado: mi-sitio.json. Ahora puedes preguntarme sobre el anÃ¡lisis.\n",
    "```\n",
    "\n",
    "**Paso 3: Usuario hace pregunta**\n",
    "\n",
    "```\n",
    "User: Â¿CuÃ¡l es el rendimiento de mi sitio?\n",
    "```\n",
    "\n",
    "**Paso 4: Sistema procesa**\n",
    "\n",
    "```\n",
    "1. Preprocesa mi-sitio.json (2.5 MB â†’ 250 KB)\n",
    "2. Resume con llama-3.1-8b-instant (250 KB â†’ 5 KB texto)\n",
    "3. EnvÃ­a resumen + pregunta a llama-3.3-70b-versatile\n",
    "4. Recibe respuesta experta\n",
    "```\n",
    "\n",
    "**Paso 5: Respuesta al usuario**\n",
    "\n",
    "```\n",
    "Assistant: Basado en el reporte de mi-sitio.json, tu sitio tiene un rendimiento de 65/100.\n",
    "\n",
    "Problemas principales:\n",
    "â€¢ LCP (Largest Contentful Paint): 3.2s - Necesita optimizaciÃ³n\n",
    "â€¢ JavaScript no usado: 250 KiB - Oportunidad de mejora\n",
    "â€¢ Contraste de color: 5 elementos con problemas\n",
    "\n",
    "Recomendaciones prioritarias:\n",
    "1. Optimizar imÃ¡genes para reducir LCP\n",
    "2. Eliminar JavaScript no usado\n",
    "3. Mejorar contraste de colores para accesibilidad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: MÃºltiples Reportes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usuario sube 3 reportes:**\n",
    "- `produccion.json`\n",
    "- `staging.json`\n",
    "- `desarrollo.json`\n",
    "\n",
    "**Chat muestra:**\n",
    "```\n",
    "Assistant: ğŸ“Š 3 reportes cargados. Puedo analizar cualquiera de ellos.\n",
    "```\n",
    "\n",
    "**Usuario pregunta:**\n",
    "```\n",
    "User: Compara el rendimiento entre producciÃ³n y staging\n",
    "```\n",
    "\n",
    "**Sistema:**\n",
    "```\n",
    "1. Preprocesa ambos reportes\n",
    "2. Resume ambos (2 llamadas a 8b-instant)\n",
    "3. EnvÃ­a ambos resÃºmenes al modelo grande\n",
    "4. Modelo hace anÃ¡lisis comparativo\n",
    "```\n",
    "\n",
    "**Respuesta:**\n",
    "```\n",
    "Assistant: ComparaciÃ³n entre producciÃ³n y staging:\n",
    "\n",
    "Performance:\n",
    "â€¢ ProducciÃ³n: 65/100\n",
    "â€¢ Staging: 78/100 âœ“\n",
    "\n",
    "Staging estÃ¡ mejor optimizado:\n",
    "- LCP: 2.1s vs 3.2s en producciÃ³n\n",
    "- JavaScript: 150 KiB vs 250 KiB en producciÃ³n\n",
    "\n",
    "RecomendaciÃ³n: Aplicar las optimizaciones de staging a producciÃ³n.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3: Eliminar Reporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usuario hace clic en ğŸ—‘ï¸ junto a `desarrollo.json`**\n",
    "\n",
    "```\n",
    "Sistema:\n",
    "  1. Elimina de session_state.lighthouse_reports\n",
    "  2. Activa flag report_removed\n",
    "  3. Actualiza sidebar: \"âœ… 2 reporte(s) cargado(s)\"\n",
    "```\n",
    "\n",
    "**Chat muestra:**\n",
    "```\n",
    "Assistant: ğŸ—‘ï¸ Reporte eliminado. Quedan 2 reporte(s) cargado(s).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen Final\n",
    "\n",
    "### Mejoras Implementadas\n",
    "\n",
    "1. âœ… **Sistema de carga de reportes** con validaciÃ³n y gestiÃ³n visual\n",
    "2. âœ… **Preprocesamiento inteligente** que reduce 20-95% el tamaÃ±o de reportes\n",
    "3. âœ… **Resumen automÃ¡tico con LLM** que convierte JSON a texto natural\n",
    "4. âœ… **Arquitectura multi-modelo** optimizada para costo y calidad\n",
    "5. âœ… **Notificaciones en tiempo real** para mejor UX\n",
    "6. âœ… **Manejo robusto de errores** con fallbacks\n",
    "\n",
    "### Impacto en el Usuario\n",
    "\n",
    "- **MÃ¡s rÃ¡pido**: Procesamiento optimizado con modelo pequeÃ±o\n",
    "- **MÃ¡s econÃ³mico**: Reduce tokens enviados al modelo principal\n",
    "- **MÃ¡s escalable**: Maneja reportes de cualquier tamaÃ±o\n",
    "- **Mejor experiencia**: Feedback visual inmediato\n",
    "- **MÃ¡s preciso**: El modelo recibe informaciÃ³n pre-procesada y estructurada\n",
    "\n",
    "### Arquitectura Final\n",
    "\n",
    "```\n",
    "Usuario â†’ Sube JSON â†’ Preprocesa â†’ Resume (8b) â†’ Analiza (70b) â†’ Respuesta\n",
    "  2.5 MB      â†“         250 KB        â†“  5 KB         â†“\n",
    "          Valida JSON   Elimina    Convierte    AnÃ¡lisis\n",
    "                        datos      a texto      experto\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
