{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525379ea",
   "metadata": {},
   "source": [
    "# ASISTENTE INTELIGENTE CON LLM Y RAG PARA LA OPTIMIZACIÓN WEB: PROPUESTA Y PLANIFICACIÓN\n",
    "\n",
    "## 1. PROBLEMA\n",
    "\n",
    "El rendimiento, la accesibilidad y el posicionamiento en motores de búsqueda (SEO) son factores clave en el desarrollo de sitios web modernos. Estos elementos garantizan una mejor experiencia de usuario y una mayor visibilidad en línea. Herramientas como Google Lighthouse permiten generar reportes automáticos que evalúan estos factores, identificando métricas clave y áreas de mejora.\n",
    "Sin embargo, estos reportes suelen ser extensos, técnicos y poco accesibles para desarrolladores con poca experiencia o para equipos de desarrollo que requieren recomendaciones prácticas y priorizadas. Aunque Google Lighthouse indica qué aspectos deben optimizarse, no siempre proporciona guías accionables adaptadas al contexto específico de cada proyecto, lo que dificulta la toma de decisiones efectivas y la implementación de mejoras.\n",
    "La falta de un asistente que interprete los reportes, traduzca los hallazgos en acciones concretas y priorice las recomendaciones según su impacto y esfuerzo, provoca retrasos en los ciclos de desarrollo, posibles malas prácticas en la corrección y, en consecuencia, sitios web con bajo rendimiento, accesibilidad reducida y posicionamiento deficiente.\n",
    "Por lo tanto, existe la necesidad de una aplicación que, apoyada en modelos de lenguaje grande (LLM) y técnicas de recuperación aumentada de información (RAG), actúe como asistente inteligente capaz de analizar los reportes de Google Lighthouse y ofrecer recomendaciones claras, prácticas y priorizadas, facilitando la optimización integral de sitios web de manera más eficiente.\n",
    "\n",
    "## 2. OBJETIVOS\n",
    "### 2.1. Objetivo general\n",
    "\n",
    "Desarrollar un asistente inteligente basado en LLM y RAG capaz de interpretar reportes de Google Lighthouse y generar recomendaciones prácticas, priorizadas y accionables para mejorar el rendimiento, la accesibilidad y el SEO de sitios web.\n",
    "\n",
    "### 2.2. Objetivos específicos\n",
    "\n",
    "1. Diseñar un mecanismo de procesamiento que permita analizar y extraer información relevante de los reportes JSON generados por Google Lighthouse.\n",
    "2.\tIntegrar un LLM con soporte RAG, que interprete los hallazgos y los traduzca en recomendaciones claras y adaptadas al contexto del sitio web evaluado.\n",
    "3.\tDesarrollar una interfaz de usuario sencilla e intuitiva que permita cargar reportes, visualizar los resultados priorizados y exportar un plan de optimización en formato accesible.\n",
    "\n",
    "## 3. JUSTIFICACIÓN\n",
    "\n",
    "La optimización del rendimiento, la accesibilidad y el SEO en aplicaciones web es un desafío constante para los desarrolladores y equipos de tecnología. Aunque Google Lighthouse provee un análisis detallado de estos aspectos, sus reportes pueden resultar extensos, técnicos y difíciles de interpretar, especialmente para quienes no cuentan con experiencia avanzada en métricas web o buenas prácticas de optimización.\n",
    "Un modelo de lenguaje grande (LLM), combinado con técnicas de recuperación aumentada de información (RAG), representa una solución adecuada a este problema por varias razones:\n",
    "\n",
    "-\tLos LLMs pueden procesar y explicar información técnica de los reportes de Lighthouse en un lenguaje más claro y comprensible.\n",
    "-\tMediante RAG, el asistente puede acceder a guías, buenas prácticas y ejemplos de implementación que complementen la información del reporte, ofreciendo sugerencias accionables y adaptadas al entorno del proyecto.\n",
    "-\tUn LLM puede no solo listar problemas, sino también organizarlos en función de impacto, esfuerzo y beneficio esperado, facilitando la toma de decisiones.\n",
    "-\tLa solución permite que tanto desarrolladores expertos como principiantes puedan entender rápidamente las mejoras necesarias, reduciendo la curva de aprendizaje y acelerando el ciclo de optimización.\n",
    "\n",
    "De esta manera, el uso de un LLM con RAG aporta un valor significativo al transformar un reporte técnico en un plan práctico de mejora, mejorando la eficiencia del desarrollo web y aumentando la calidad de los sitios en términos de rendimiento, accesibilidad y posicionamiento en buscadores.\n",
    "\n",
    "## 4. MODELO LLM\n",
    "\n",
    "El modelo seleccionado para el desarrollo de la aplicación es CodeLlama 7B de Meta, ejecutado de forma local a través de Ollama. CodeLlama es una variante de la familia Llama optimizado para tareas de programación y análisis técnico, lo que lo convierte en una opción adecuada para interpretar y explicar reportes de Google Lighthouse.\n",
    "\n",
    "### 4.1. Justificación de la elección\n",
    "\n",
    "-\tCodeLlama ha sido entrenado específicamente en datos de programación, lo que le permite comprender métricas, estructuras de datos y recomendaciones técnicas con mayor precisión que un modelo general.\n",
    "-\tLa posibilidad de correr el modelo en local mejora la privacidad de los datos analizados (reportes de Lighthouse) y evita depender de servicios externos en la nube.\n",
    "-\tLa versión 7B ofrece una buena relación entre consumo de recursos y velocidad de inferencia, siendo adecuada para prototipos rápidos y equipos con hardware limitado.\n",
    "-\tCodeLlama puede integrarse con técnicas de recuperación aumentada de información para complementar sus respuestas con guías, ejemplos y mejores prácticas almacenadas en la base de conocimiento del proyecto.\n",
    "\n",
    "## 5. DIAGRAMA DE ARQUITECTURA\n",
    "![Diagrama de Arquitectura](./diagrams/architecture_diagram.png)\n",
    "La arquitectura propuesta sigue un enfoque modular compuesta por los siguientes elementos:\n",
    "1.\tInterfaz de Usuario (UI): desarrollada en Next.js, permite al usuario cargar los reportes JSON de Google Lighthouse y visualizar los resultados generados por el asistente de manera clara.\n",
    "2.\tBackend: actúa como intermediario entre la interfaz y el modelo. Se encarga de procesar los reportes de Lighthouse, generar los prompts necesarios y organizar la respuesta recibida.\n",
    "3.\tOllama Runtime: proporciona el entorno local para ejecutar el modelo de lenguaje CodeLlama, asegurando privacidad en el procesamiento y menor dependencia de servicios externos.\n",
    "4.\tModelo LLM (CodeLlama 7B): núcleo de la solución, encargado de interpretar los hallazgos de los reportes y generar recomendaciones contextualizadas y priorizadas.\n",
    "5.\tMódulo RAG (Recuperación Aumentada de Información): consulta una base de conocimiento compuesta por guías, buenas prácticas y ejemplos técnicos, para enriquecer las respuestas del modelo con información práctica y confiable.\n",
    "El flujo de interacción es el siguiente: el usuario carga un reporte → el backend lo procesa → se consulta la base de conocimiento mediante RAG → se construye un prompt enriquecido → Ollama ejecuta el modelo CodeLlama → el backend organiza la salida y la muestra en la interfaz.\n",
    "\n",
    "## 6. PRESENTACIÓN\n",
    "La presentación de la primera entrega se puede observar en la siguiente URL:\n",
    "-\t[Figma | Asistente Google Lighthouse](https://www.figma.com/deck/BSqKzByYz6St0e0Ztx9qSx/Asistente-Google-Lighthouse?node-id=1-108&t=HU5ZIqNsKvDw8NSS-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
